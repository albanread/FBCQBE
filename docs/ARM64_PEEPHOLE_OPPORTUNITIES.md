# ARM64 Peephole Optimization Opportunities

This document analyzes actual ARM64 assembly generated by FasterBASIC tests to identify real optimization opportunities.

## Analysis Date
Generated from test suite analysis - December 2024

## Current Status
- **MADD/MSUB Fusion**: âœ… Implemented and working
- **Shifted Operand Fusion**: âœ… Implemented (ready for immediate shifts)
- **Other optimizations**: ðŸ“‹ Documented below

---

## 1. ðŸ”´ HIGH PRIORITY: Redundant Loop Condition Checks

### Pattern Found
```assembly
cmp	x19, #10
cset	w0, le          ; Set w0 = (x19 <= 10)
cmp	x19, #10        ; âŒ REDUNDANT! Same comparison again
cset	w1, ge          ; Set w1 = (x19 >= 10)
mov	w2, #0
and	w1, w1, w2      ; w1 = w1 & 0 (always 0)
orr	w0, w0, w1      ; w0 = w0 | 0 (no-op)
cmp	w0, #0
beq	L6
```

### Issue
This appears in **every FOR loop** in the generated code. The comparison `x19 <= 10` is done twice, and there's unnecessary boolean logic that could be simplified.

### Root Cause
This is generated by the FasterBASIC frontend for FOR loop conditions. The IL probably contains redundant comparisons.

### Potential Fix
**Frontend optimization** (better) - Generate simpler loop conditions in QBE IL:
- Use single comparison instead of two
- Simplify boolean logic before emitting IL

**Peephole optimization** (harder) - Detect and eliminate:
1. Duplicate CMP instructions
2. Dead boolean operations (AND with 0, OR with 0)
3. CSET + CMP patterns that can be simplified to direct branch

### Impact
- Appears in: All FOR loops
- Savings: ~6 instructions per loop iteration â†’ ~2 instructions
- Performance: Moderate (loops are hot paths)

---

## 2. ðŸŸ¡ MEDIUM PRIORITY: Repeated ADRP+ADD for Same Address

### Pattern Found
```assembly
; Loading _data_str.1 eighteen times in one function:
adrp	x0, _data_str.1@page
add	x0, x0, _data_str.1@pageoff
bl	_string_new_utf8
; ... later in same function ...
adrp	x0, _data_str.1@page           ; âŒ Same address loaded again
add	x0, x0, _data_str.1@pageoff
bl	_string_new_utf8
```

### Issue
The same string literal address `_data_str.1` is loaded 18 times using ADRP+ADD in a single function (`test_integer_basic.bas`).

### Potential Fix
**Register allocation / common subexpression elimination** (better) - QBE should recognize these as common subexpressions and cache in a register.

**Peephole optimization** (limited) - Track recently computed addresses within a basic block and reuse them if the register is still live.

**Frontend optimization** - Cache string descriptor creation results when printing the same string multiple times.

### Impact
- Appears in: String-heavy code (test output, printing)
- Savings: 2 instructions per repeated load
- In `test_integer_basic.bas`: Could save ~34 instructions (17 repeated loads Ã— 2)
- Performance: Low to moderate (depends on string usage patterns)

---

## 3. ðŸŸ¡ MEDIUM PRIORITY: Array Descriptor Field Access

### Pattern Found
```assembly
; Loading two fields from same array descriptor:
adrp	x0, _arr_A_INT_global@page+8
add	x0, x0, _arr_A_INT_global@pageoff+8
ldr	x21, [x0]                      ; Load lower_bound

adrp	x0, _arr_A_INT_global@page+16   ; âŒ Same base address
add	x0, x0, _arr_A_INT_global@pageoff+16
ldr	x2, [x0]                       ; Load upper_bound
```

### Issue
Array descriptors have multiple fields (base, lower_bound, upper_bound, element_size). Each field access recomputes the base address with ADRP+ADD.

### Potential Fix
**Option 1**: Load array descriptor base address once, use offset addressing:
```assembly
adrp	x0, _arr_A_INT_global@page
add	x0, x0, _arr_A_INT_global@pageoff
ldr	x21, [x0, #8]   ; lower_bound
ldr	x2, [x0, #16]   ; upper_bound
```

**Option 2**: Use load-pair (LDP) when loading consecutive fields:
```assembly
adrp	x0, _arr_A_INT_global@page
add	x0, x0, _arr_A_INT_global@pageoff
ldp	x21, x2, [x0, #8]   ; Load both bounds in one instruction
```

**Implementation**: This would be a **peephole optimization** that detects consecutive loads from the same base address and combines them.

### Impact
- Appears in: Every array access (very common)
- Savings: 2-4 instructions per array access
- Performance: Moderate to high (arrays are common, LDP has better throughput)

---

## 4. ðŸ”´ HIGH PRIORITY: MADD Fusion Blocked by Intervening Instructions

### Pattern Found
```assembly
; Array index calculation: (index - lower_bound) * element_size + base_ptr
sub	x0, x20, x21              ; index - lower_bound
adrp	x1, _arr_A_INT_global@page+40   ; âŒ These instructions...
add	x1, x1, _arr_A_INT_global@pageoff+40
ldr	x1, [x1]                  ; ...block MADD fusion
mul	x0, x0, x1                ; offset = adjusted_index * element_size
adrp	x1, _arr_A_INT_global@page      ; âŒ More intervening instructions
add	x1, x1, _arr_A_INT_global@pageoff
ldr	x1, [x1]                  ; base_ptr
add	x1, x0, x1                ; final_address = offset + base_ptr
```

### Issue
The MUL and ADD are not adjacent due to intervening ADRP+ADD+LDR sequences. Our MADD fusion only works on adjacent instructions.

### Potential Fix
**Option 1**: Improve register allocation/scheduling in QBE to keep MUL and ADD adjacent.

**Option 2**: More sophisticated peephole that can fuse across some safe intervening instructions (complex and risky).

**Option 3**: Frontend optimization - reorganize array access IL to group operations better.

**Option 4**: Load array descriptor fields into registers earlier in the basic block, before the calculation starts.

### Impact
- Appears in: Every array access calculation
- Savings: 1 instruction per array access (MUL+ADD â†’ MADD)
- Performance: Moderate (arrays are common)

---

## 5. ðŸŸ¢ LOW PRIORITY: MOV Before Bounds Check

### Pattern Found
```assembly
cmp	w0, #0
bne	L5
mov	x1, x21        ; âŒ Could be done earlier or avoided
mov	x0, x20
bl	_basic_array_bounds_error
```

### Issue
The MOVs to set up error function arguments only happen on the error path. This is actually good for the happy path, but could potentially be optimized away entirely.

### Potential Fix
**Better**: Keep this as-is. The MOVs only execute when there's an error (rare path), so they don't hurt performance.

**Alternative**: If bounds checking is disabled, these are dead code.

### Impact
- Low priority - only affects error path
- No optimization recommended

---

## 6. ðŸŸ¢ LOW PRIORITY: Shift with Register Amount (Not Immediate)

### Pattern Found
```assembly
mov	w0, #2
lsl	x0, x19, x0     ; Shift by register, not immediate
mov	x1, #2
add	x0, x0, x1
```

### Issue
The shift amount is loaded into a register instead of being an immediate. This prevents our shifted-operand fusion from working.

### Root Cause
The FasterBASIC frontend optimizes `x * 4` into a shift, but emits it as a register shift rather than an immediate shift in the QBE IL.

### Potential Fix
**Frontend optimization**: When generating shifts from constant multiplications, emit them as immediate shifts in the IL.

**QBE optimization**: Recognize constant loads followed by shift and fold into immediate shift.

### Impact
- Low priority - register shifts are fine, just miss fusion opportunity
- Savings: Would enable shifted-operand fusion (1 instruction per pattern)

---

## 7. ðŸŸ¡ MEDIUM PRIORITY: CSET + CMP + Branch Pattern

### Pattern Found
```assembly
cmp	x20, x21
cset	w0, ge          ; w0 = (x20 >= x21)
cmp	x20, x2
cset	w1, le          ; w1 = (x20 <= x2)
and	w0, w0, w1      ; w0 = (x20 >= x21) AND (x20 <= x2)
cmp	w0, #0
bne	L5              ; Branch if in range
```

### Issue
Bounds checking creates CSET instructions that materialize boolean values, then compares them. Could use conditional branches directly.

### Potential Fix
**Peephole optimization**: Transform CSET + CMP + Bcc patterns into direct conditional branches:
```assembly
cmp	x20, x21
blt	error           ; Branch if out of range (low)
cmp	x20, x2
bgt	error           ; Branch if out of range (high)
; Fall through if in range
```

**Frontend optimization**: Generate better bounds-check IL using direct branches.

### Impact
- Appears in: Every array access with bounds checking
- Savings: 3-4 instructions per bounds check
- Performance: Moderate (arrays are common)

---

## 8. ðŸ”µ FUTURE: Load-Pair Opportunities

### Pattern Found
```assembly
ldr	x19, [x29, 40]
ldr	x20, [x29, 32]
ldr	x21, [x29, 24]
ldp	x29, x30, [sp], 48
```

### Observation
Function epilogues already use LDP for x29/x30. Could potentially use LDP for other saved registers.

### Potential Fix
This is a **register save/restore** optimization, already partially implemented. Could extend to save more register pairs.

### Impact
- Low priority - function prologue/epilogue is cold
- Savings: 1-2 instructions per function
- Code size: Slightly smaller

---

## Summary of Recommendations

### Quick Wins (Frontend Fixes)
1. **Simplify FOR loop conditions** - Eliminate redundant comparisons in IL generation
2. **Use immediate shifts** - When converting `x * constant` to shifts, use immediate operands
3. **Better array access ordering** - Group descriptor loads before calculations

### Peephole Optimizations Worth Implementing
1. âœ… **MADD/MSUB fusion** - Already done
2. âœ… **Shifted operand fusion** - Already done
3. ðŸŽ¯ **Load-pair fusion** - Combine consecutive loads from same base address into LDP
4. ðŸŽ¯ **CSET + CMP elimination** - Simplify boolean materialization patterns
5. ðŸŽ¯ **ADRP base caching** - Reuse ADRP results within basic blocks

### Metrics from Analysis
- `test_integer_basic.bas`: 18 redundant ADRP+ADD sequences for same string
- Every FOR loop: 6 instructions could be reduced to 2
- Every array access: 4-6 redundant ADRP+ADD sequences
- MADD fusion blocked: Every array index calculation

### Estimated Impact
If all optimizations were implemented:
- **Code size**: 10-20% reduction in instruction count
- **Performance**: 5-15% improvement in array-heavy code
- **Biggest wins**: Loop condition simplification + array descriptor caching

---

## Practical Recommendations for Next Steps

### 1. Frontend Optimization: Simplify Loop Conditions (HIGHEST IMPACT)

**Current codegen produces:**
```
cmp x19, #10
cset w0, le
cmp x19, #10        ; Duplicate comparison
cset w1, ge
mov w2, #0
and w1, w1, w2
orr w0, w0, w1
cmp w0, #0
beq L6
```

**Should produce:**
```
cmp x19, #10
bgt L6              ; Just branch directly
```

**Where to fix:** `fsh/FasterBASICT/src/codegen/qbe_codegen_statements.cpp` in the FOR loop emission code.

**Benefit:** Reduce every FOR loop iteration by ~6 instructions. Appears in every loop in every program.

---

### 2. Frontend Optimization: Cache Array Descriptor Base Address

**Current pattern:** Load each array descriptor field with separate ADRP+ADD:
```
adrp x0, _arr_A_INT_global@page+8
add  x0, x0, _arr_A_INT_global@pageoff+8
ldr  x21, [x0]                           ; lower_bound
adrp x0, _arr_A_INT_global@page+16       ; Recalculate base!
add  x0, x0, _arr_A_INT_global@pageoff+16
ldr  x2, [x0]                            ; upper_bound
```

**Better approach:** Load base once, use offsets:
```
adrp x0, _arr_A_INT_global@page
add  x0, x0, _arr_A_INT_global@pageoff
ldr  x21, [x0, #8]    ; lower_bound
ldr  x2, [x0, #16]    ; upper_bound
ldr  x1, [x0, #40]    ; element_size
ldr  x1, [x0]         ; base_ptr
```

**Where to fix:** `fsh/FasterBASICT/src/codegen/qbe_codegen_expressions.cpp` in array access codegen. Emit one temp for the descriptor base address, then use it for all field accesses.

**Benefit:** Save 2 instructions per array descriptor field access. Every array access reads 2-4 fields, so 4-8 instructions saved per array access.

---

### 3. Peephole Optimization: LDP Fusion for Consecutive Loads

Once the frontend emits better array descriptor access (recommendation #2), consecutive loads become adjacent and can be fused:

```
ldr  x21, [x0, #8]     ldp  x21, x2, [x0, #8]
ldr  x2, [x0, #16]  â†’  (1 instruction instead of 2)
```

**Where to implement:** Add to `qbe_basic_integrated/qbe_source/arm64/emit.c` following the MADD/shift fusion pattern.

**Benefit:** 1 instruction saved per pair of consecutive loads. Better throughput (LDP has better pipeline utilization).

---

### 4. Frontend Optimization: Use Immediate Shifts

**Current:** `x * 4` becomes register shift:
```
mov  w0, #2
lsl  x0, x19, x0    ; Shift by register
```

**Better:** Emit immediate shift in QBE IL:
```
lsl  x0, x19, #2    ; Immediate shift
```

**Where to fix:** `fsh/FasterBASICT/src/codegen/qbe_codegen_expressions.cpp` in constant multiplication optimization. When emitting a shift, use immediate operand if shift amount is constant.

**Benefit:** Enables shifted-operand fusion (already implemented). Pattern `x * 4 + y` could become single `ADD x0, y, x, LSL #2`.

---

### 5. Frontend Optimization: Direct Conditional Branches for Bounds Checking

**Current:** Materialize booleans then branch:
```
cmp  x20, x21
cset w0, ge
cmp  x20, x2
cset w1, le
and  w0, w0, w1
cmp  w0, #0
bne  L5
```

**Better:** Direct conditional branches:
```
cmp  x20, x21
blt  error          ; Out of range (too low)
cmp  x20, x2
bgt  error          ; Out of range (too high)
; Fall through if in range
```

**Where to fix:** `fsh/FasterBASICT/src/codegen/qbe_codegen_expressions.cpp` in array bounds checking code.

**Benefit:** 4 instructions â†’ 2 instructions per bounds check. Every array access includes bounds checking.

---

## Implementation Priority

1. **Loop condition simplification** - Affects every loop, easiest fix
2. **Array descriptor caching** - Affects every array access, medium difficulty
3. **Bounds check optimization** - Affects every array access, medium difficulty
4. **Immediate shifts** - Small but enables other optimizations, easy fix
5. **LDP fusion peephole** - Only beneficial after #2, requires peephole code

## Measurement

To validate improvements, measure instruction count and runtime on:
- `tests/arrays/test_array_basic.bas` (heavy array usage)
- `tests/loops/test_for_comprehensive.bas` (heavy loop usage)
- `tests/rosetta/mersenne_factors.bas` (real computation)

Expected improvements:
- Array tests: 15-20% fewer instructions
- Loop tests: 10-15% fewer instructions
- Overall: 5-10% performance improvement